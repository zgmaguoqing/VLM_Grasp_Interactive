import cv2
import numpy as np
import torch
from ultralytics.models.sam import Predictor as SAMPredictor

import whisper
import json
import re
import base64
import textwrap
import queue
import time
import io

import soundfile as sf  
import sounddevice as sd
from scipy.io.wavfile import write
from pydub import AudioSegment

from openai import OpenAI  # å¯¼å…¥OpenAIå®¢æˆ·ç«¯

import logging
# ç¦ç”¨ Ultralytics çš„æ—¥å¿—è¾“å‡º
logging.getLogger("ultralytics").setLevel(logging.WARNING)


# ----------------------- åŸºç¡€å·¥å…·å‡½æ•° -----------------------

def encode_np_array(image_np):
    """å°† numpy å›¾åƒæ•°ç»„ï¼ˆBGRï¼‰ç¼–ç ä¸º base64 å­—ç¬¦ä¸²"""
    success, buffer = cv2.imencode('.jpg', image_np)
    if not success:
        raise ValueError("æ— æ³•å°†å›¾åƒæ•°ç»„ç¼–ç ä¸º JPEG")
    img_base64 = base64.b64encode(buffer).decode('utf-8')
    return img_base64



# ----------------------- å¤šæ¨¡æ€æ¨¡å‹è°ƒç”¨ï¼ˆQwenï¼‰ -----------------------

def generate_robot_actions(user_command, image_input=None):
    """
    ä½¿ç”¨ base64 çš„æ–¹å¼å°† numpy å›¾åƒå’Œç”¨æˆ·æ–‡æœ¬æŒ‡ä»¤ä¼ ç»™ Qwen å¤šæ¨¡æ€æ¨¡å‹ï¼Œ
    è¦æ±‚æ¨¡å‹è¿”å›ä¸¤éƒ¨åˆ†ï¼š
      - æ¨¡å‹è¿”å›å†…å®¹ä¸­ï¼Œç¬¬ä¸€éƒ¨åˆ†ä¸ºè‡ªç„¶è¯­è¨€å“åº”ï¼ˆè¯´æ˜ä¸ºä½•é€‰æ‹©è¯¥ç‰©ä½“ï¼‰ï¼Œ
      - ç´§è·Ÿå…¶åçš„éƒ¨åˆ†ä¸ºçº¯ JSON å¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

        {
          "name": "ç‰©ä½“åç§°",
          "bbox": [å·¦ä¸Šè§’x, å·¦ä¸Šè§’y, å³ä¸‹è§’x, å³ä¸‹è§’y]
        }

    è¿”å›ä¸€ä¸ª dictï¼ŒåŒ…å« "response" å’Œ "coordinates"ã€‚
    å‚æ•° image_input ä¸º numpy æ•°ç»„ï¼ˆBGR æ ¼å¼ï¼‰ã€‚
    """
    # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
    # æ›¿æ¢ä¸ºè‡ªå·±çš„æ¨¡å‹è°ƒç”¨ï¼Œæ²¡æœ‰æœ¬åœ°éƒ¨ç½²çš„ï¼Œå¯ä»¥å‚è€ƒè¯¥ç½‘ç«™ https://sg.uiuiapi.com/v1
    client = OpenAI(api_key='sk-21a42456345b4b6da1df9d0a08a4396c', base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")

    system_prompt = textwrap.dedent("""\
    ä½ æ˜¯ä¸€ä¸ªç²¾å¯†æœºæ¢°è‡‚è§†è§‰æ§åˆ¶ç³»ç»Ÿï¼Œå…·å¤‡å…ˆè¿›çš„å¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œä»»åŠ¡ï¼š

    ã€å›¾åƒåˆ†æé˜¶æ®µã€‘
    1. åˆ†æè¾“å…¥å›¾åƒï¼Œè¯†åˆ«å›¾åƒä¸­æ‰€æœ‰å¯è§ç‰©ä½“ï¼Œå¹¶è®°å½•æ¯ä¸ªç‰©ä½“çš„è¾¹ç•Œæ¡†ï¼ˆå·¦ä¸Šè§’ç‚¹å’Œå³ä¸‹è§’ç‚¹ï¼‰åŠå…¶ç±»åˆ«åç§°ã€‚

    ã€æŒ‡ä»¤è§£æé˜¶æ®µã€‘
    2. æ ¹æ®ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œä»è¯†åˆ«çš„ç‰©ä½“ä¸­ç­›é€‰å‡ºæœ€åŒ¹é…çš„ç›®æ ‡ç‰©ä½“ã€‚

    ã€å“åº”ç”Ÿæˆé˜¶æ®µã€‘
    3. è¾“å‡ºæ ¼å¼å¿…é¡»ä¸¥æ ¼å¦‚ä¸‹ï¼š
    - è‡ªç„¶è¯­è¨€å“åº”ï¼ˆä»…åŒ…å«è¯´æ˜ä¸ºä½•é€‰æ‹©è¯¥ç‰©ä½“çš„æ–‡å­—,å¯ä»¥ä¿çš®å¯çˆ±åœ°å›åº”ç”¨æˆ·çš„éœ€æ±‚ï¼Œä½†æ˜¯è¯·æ³¨æ„ï¼Œå›ç­”ä¸­åº”è¯¥åªåŒ…å«è¢«é€‰ä¸­çš„ç‰©ä½“ï¼‰ï¼Œ
    - ç´§è·Ÿå…¶åï¼Œä»ä¸‹ä¸€è¡Œå¼€å§‹è¿”å› **æ ‡å‡† JSON å¯¹è±¡**,ä½†æ˜¯ä¸è¦è¿”å›jsonæœ¬ä½“,æ ¼å¼å¦‚ä¸‹ï¼š

    {
      "name": "ç‰©ä½“åç§°",
      "bbox": [å·¦ä¸Šè§’x, å·¦ä¸Šè§’y, å³ä¸‹è§’x, å³ä¸‹è§’y]
    }

    ã€æ³¨æ„äº‹é¡¹ã€‘
    - JSON å¿…é¡»ä»ä¸‹ä¸€è¡Œå¼€å§‹ï¼›
    - è‡ªç„¶è¯­è¨€å“åº”ä¸ JSON ä¹‹é—´æ— å…¶ä»–é¢å¤–æ–‡æœ¬;
    - JSON å¯¹è±¡ä¸èƒ½æœ‰ä»»ä½•æ³¨é‡Šã€é¢å¤–æ–‡æœ¬æˆ–è§£é‡Š,åŒ…æ‹¬ä¸èƒ½æœ‰è¾…åŠ©æ ‡è¯†ä¸ºjsonæ–‡æœ¬çš„å†…å®¹,ä¸è¦æœ‰json;
    - åæ ‡ bbox å¿…é¡»ä¸ºæ•´æ•°ï¼›
    - åªå…è®¸ä½¿ç”¨ "bbox" ä½œä¸ºåæ ‡æ ¼å¼ã€‚
    """)

    messages = [{"role": "system", "content": system_prompt}]
    user_content = []

    if image_input is not None:
        base64_img = encode_np_array(image_input)
        user_content.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{base64_img}"
            }
        })

    user_content.append({"type": "text", "text": user_command})
    messages.append({"role": "user", "content": user_content})

    try:
        # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è°ƒç”¨API
        completion = client.chat.completions.create(
            # model="qwen-vl-plus",  # æŒ‡å®šæ¨¡å‹åç§°ï¼Œè¯·ç¡®è®¤æœåŠ¡æä¾›å•†æ”¯æŒçš„æ¨¡å‹å
            model="qwen-vl-max", 
            # model="gpt-5",
            # model="qwen2.5-vl-32b-instruct",
            messages=messages,
            # max_tokens=4096,  # å¯æ ¹æ®éœ€è¦è°ƒæ•´
            temperature=0.1,   # é™ä½æ¸©åº¦ä»¥æé«˜è¾“å‡ºçš„ç¡®å®šæ€§ï¼Œå¯¹ç»“æ„åŒ–è¾“å‡ºæœ‰ç›Š
        )
        
        content = completion.choices[0].message.content
        print("åŸå§‹å“åº”ï¼š", content)

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾ JSON éƒ¨åˆ†
        match = re.search(r'(\{.*\})', content, re.DOTALL)
        if match:
            json_str = match.group(1)
            try:
                coord = json.loads(json_str)
            except Exception as e:
                print(f"[è­¦å‘Š] JSON è§£æå¤±è´¥ï¼š{e}")
                coord = {}
            natural_response = content[:match.start()].strip()
        else:
            natural_response = content.strip()
            coord = {}

        return {
            "response": natural_response,
            "coordinates": coord
        }

    except Exception as e:
        print(f"è¯·æ±‚å¤±è´¥ï¼š{e}")
        return {"response": "å¤„ç†å¤±è´¥", "coordinates": {}}
# -----------
# import base64
# import json
# import re
# import textwrap
# from typing import Dict, Any, Optional, List

# import numpy as np

# # ============== ä¾èµ–ï¼ˆHF æœ¬åœ°æ¨ç†ï¼‰ ==============
# # pip install "transformers>=4.44.0" accelerate torch pillow
# import torch
# from PIL import Image
# from transformers import Qwen3VLForConditionalGeneration, AutoProcessor

# # ï¼ˆå¯é€‰ï¼‰ä»ä¿ç•™ OpenAI-å…¼å®¹è·¯å¾„ï¼›å¦‚æœåªç”¨ HFï¼Œå¯åˆ é™¤æœ¬æ®µä¾èµ–ä¸å‡½æ•°
# try:
#     from openai import OpenAI
# except Exception:
#     OpenAI = None


# # ---------------- å…¬å…±å·¥å…·å‡½æ•° ----------------
# def encode_np_array(img_bgr: np.ndarray) -> str:
#     """
#     å°† BGR numpy æ•°ç»„ç¼–ç ä¸º JPEG -> base64 å­—ç¬¦ä¸²ï¼ˆç”¨äº OpenAI-å…¼å®¹åç«¯ï¼‰ã€‚
#     HF åç«¯ä¸éœ€è¦ base64ï¼Œç›´æ¥ç”¨ PIL å³å¯ã€‚
#     """
#     import cv2
#     ok, buf = cv2.imencode(".jpg", img_bgr)
#     if not ok:
#         raise ValueError("å›¾åƒç¼–ç å¤±è´¥")
#     return base64.b64encode(buf.tobytes()).decode("utf-8")


# def bgr_to_pil(img_bgr: np.ndarray) -> Image.Image:
#     """
#     å°† BGR numpy æ•°ç»„è½¬æ¢ä¸º PIL(RGB)ã€‚
#     """
#     if img_bgr is None:
#         return None
#     img_rgb = img_bgr[..., ::-1].copy()
#     return Image.fromarray(img_rgb)


# def _build_system_prompt() -> str:
#     return textwrap.dedent("""\
#     ä½ æ˜¯ä¸€ä¸ªç²¾å¯†æœºæ¢°è‡‚è§†è§‰æ§åˆ¶ç³»ç»Ÿï¼Œå…·å¤‡å…ˆè¿›çš„å¤šæ¨¡æ€æ„ŸçŸ¥èƒ½åŠ›ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œä»»åŠ¡ï¼š

#     ã€å›¾åƒåˆ†æé˜¶æ®µã€‘
#     1. åˆ†æè¾“å…¥å›¾åƒï¼Œè¯†åˆ«å›¾åƒä¸­æ‰€æœ‰å¯è§ç‰©ä½“ï¼Œå¹¶è®°å½•æ¯ä¸ªç‰©ä½“çš„è¾¹ç•Œæ¡†ï¼ˆå·¦ä¸Šè§’ç‚¹å’Œå³ä¸‹è§’ç‚¹ï¼‰åŠå…¶ç±»åˆ«åç§°ã€‚

#     ã€æŒ‡ä»¤è§£æé˜¶æ®µã€‘
#     2. æ ¹æ®ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œä»è¯†åˆ«çš„ç‰©ä½“ä¸­ç­›é€‰å‡ºæœ€åŒ¹é…çš„ç›®æ ‡ç‰©ä½“ã€‚

#     ã€å“åº”ç”Ÿæˆé˜¶æ®µã€‘
#     3. è¾“å‡ºæ ¼å¼å¿…é¡»ä¸¥æ ¼å¦‚ä¸‹ï¼š
#     - è‡ªç„¶è¯­è¨€å“åº”ï¼ˆä»…åŒ…å«è¯´æ˜ä¸ºä½•é€‰æ‹©è¯¥ç‰©ä½“çš„æ–‡å­—,å¯ä»¥ä¿çš®å¯çˆ±åœ°å›åº”ç”¨æˆ·çš„éœ€æ±‚ï¼Œä½†æ˜¯è¯·æ³¨æ„ï¼Œå›ç­”ä¸­åº”è¯¥åªåŒ…å«è¢«é€‰ä¸­çš„ç‰©ä½“ï¼‰ï¼Œ
#     - ç´§è·Ÿå…¶åï¼Œä»ä¸‹ä¸€è¡Œå¼€å§‹è¿”å› **æ ‡å‡† JSON å¯¹è±¡**,ä½†æ˜¯ä¸è¦è¿”å›jsonæœ¬ä½“,æ ¼å¼å¦‚ä¸‹ï¼š

#     {
#       "name": "ç‰©ä½“åç§°",
#       "bbox": [å·¦ä¸Šè§’x, å·¦ä¸Šè§’y, å³ä¸‹è§’x, å³ä¸‹è§’y]
#     }

#     ã€æ³¨æ„äº‹é¡¹ã€‘
#     - JSON å¿…é¡»ä»ä¸‹ä¸€è¡Œå¼€å§‹ï¼›
#     - è‡ªç„¶è¯­è¨€å“åº”ä¸ JSON ä¹‹é—´æ— å…¶ä»–é¢å¤–æ–‡æœ¬;
#     - JSON å¯¹è±¡ä¸èƒ½æœ‰ä»»ä½•æ³¨é‡Šã€é¢å¤–æ–‡æœ¬æˆ–è§£é‡Š,åŒ…æ‹¬ä¸èƒ½æœ‰è¾…åŠ©æ ‡è¯†ä¸ºjsonæ–‡æœ¬çš„å†…å®¹,ä¸è¦æœ‰json;
#     - åæ ‡ bbox å¿…é¡»ä¸ºæ•´æ•°ï¼›
#     - åªå…è®¸ä½¿ç”¨ "bbox" ä½œä¸ºåæ ‡æ ¼å¼ã€‚
#     """)


# def _extract_text_and_json(content: str) -> Dict[str, Any]:
#     """
#     ä»æ¨¡å‹è¾“å‡ºä¸­åˆ†ç¦»è‡ªç„¶è¯­è¨€éƒ¨åˆ†ä¸ JSON å¯¹è±¡ã€‚
#     """
#     match = re.search(r'(\{.*\})', content, re.DOTALL)
#     if match:
#         json_str = match.group(1)
#         try:
#             coord = json.loads(json_str)
#         except Exception as e:
#             print(f"[è­¦å‘Š] JSON è§£æå¤±è´¥ï¼š{e}")
#             coord = {}
#         natural_response = content[:match.start()].strip()
#     else:
#         natural_response = content.strip()
#         coord = {}

#     return {"response": natural_response, "coordinates": coord}


# # ---------------- OpenAI-å…¼å®¹åç«¯ï¼ˆå¯é€‰ä¿ç•™ï¼‰ ----------------
# def _call_openai_compatible(
#     user_command: str,
#     image_b64: Optional[str],
#     model: str,
#     api_key: str = "",
#     base_url: str = ""
# ) -> Dict[str, Any]:
#     if OpenAI is None:
#         raise RuntimeError("æœªå®‰è£… openai åŒ…ï¼Œè¯· `pip install openai`ã€‚")
#     client = OpenAI(api_key=api_key or "EMPTY", base_url=base_url or None)

#     system_prompt = _build_system_prompt()
#     messages: List[Dict[str, Any]] = [{"role": "system", "content": system_prompt}]

#     user_content: List[Dict[str, Any]] = []
#     if image_b64 is not None:
#         user_content.append({
#             "type": "image_url",
#             "image_url": {"url": f"data:image/jpeg;base64,{image_b64}"}
#         })
#     user_content.append({"type": "text", "text": user_command})
#     messages.append({"role": "user", "content": user_content})

#     completion = client.chat.completions.create(
#         model=model,
#         messages=messages,
#         temperature=0.1
#     )
#     content = completion.choices[0].message.content
#     print("åŸå§‹å“åº”ï¼š", content)
#     return _extract_text_and_json(content)


# # ---------------- HF(Qwen3-VL) æœ¬åœ°åç«¯ ----------------
# _HF_STATE: Dict[str, Any] = {"model": None, "processor": None, "model_name": None}


# def _ensure_hf_qwen3(
#     hf_model_name: str,
#     dtype: str = "auto",
#     device_map: str = "auto",
#     attn_implementation: Optional[str] = None
# ):
#     """
#     å»¶è¿ŸåŠ è½½/ç¼“å­˜ Qwen3-VL æ¨¡å‹ä¸å¤„ç†å™¨ã€‚
#     """
#     if _HF_STATE["model"] is not None and _HF_STATE["model_name"] == hf_model_name:
#         return

#     kwargs = dict(dtype=dtype, device_map=device_map)
#     if attn_implementation:
#         kwargs["attn_implementation"] = attn_implementation

#     model = Qwen3VLForConditionalGeneration.from_pretrained(
#         hf_model_name, **kwargs
#     )
#     processor = AutoProcessor.from_pretrained(hf_model_name)

#     _HF_STATE["model"] = model
#     _HF_STATE["processor"] = processor
#     _HF_STATE["model_name"] = hf_model_name


# def _call_hf_qwen3(
#     user_command: str,
#     image_bgr: Optional[np.ndarray],
#     hf_model_name: str = "Qwen/Qwen3-VL-8B-Instruct",
#     max_new_tokens: int = 256,
#     attn_implementation: Optional[str] = None,
# ) -> Dict[str, Any]:
#     """
#     ä½¿ç”¨æœ¬åœ° Transformers ç›´æ¥è°ƒç”¨ Qwen3-VLã€‚
#     - å›¾åƒï¼šä¼ å…¥ numpy BGRï¼›å†…éƒ¨è½¬ PIL(RGB)
#     - æ–‡æœ¬ï¼šéµå¾ªä¸åŸå‡½æ•°ä¸€è‡´çš„ system prompt ä¸æŒ‡ä»¤
#     """
#     _ensure_hf_qwen3(
#         hf_model_name=hf_model_name,
#         dtype="auto",
#         device_map="auto",
#         attn_implementation=attn_implementation,
#     )
#     model = _HF_STATE["model"]
#     processor = _HF_STATE["processor"]

#     system_prompt = _build_system_prompt()

#     # ç»„è£… messagesï¼ˆQwen3-VL çš„èŠå¤©æ¨¡æ¿ï¼‰
#     content_list: List[Dict[str, Any]] = []
#     if image_bgr is not None:
#         pil_img = bgr_to_pil(image_bgr)
#         content_list.append({"type": "image", "image": pil_img})
#     content_list.append({"type": "text", "text": user_command})

#     messages = [
#         {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
#         {"role": "user",   "content": content_list},
#     ]

#     # ç¼–ç 
#     inputs = processor.apply_chat_template(
#         messages,
#         tokenize=True,
#         add_generation_prompt=True,
#         return_tensors="pt",
#         return_dict=True
#     )
#     # å°†è§†è§‰å¼ é‡ç­‰ç§»åŠ¨åˆ°ç›¸åŒè®¾å¤‡
#     inputs = {k: v.to(model.device) if hasattr(v, "to") else v for k, v in inputs.items()}

#     # ç”Ÿæˆï¼ˆä¸ºä¿è¯ç»“æ„åŒ–ç¨³å®šï¼Œå…³é—­é‡‡æ ·ï¼‰
#     with torch.no_grad():
#         generated_ids = model.generate(
#             **inputs,
#             max_new_tokens=max_new_tokens,
#             do_sample=False,
#             use_cache=True
#         )

#     # åªè§£ç æ–°å¢çš„ tokenï¼ˆå»æ‰æç¤ºéƒ¨åˆ†ï¼‰
#     trimmed = [
#         out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs["input_ids"], generated_ids)
#     ]
#     output_text = processor.batch_decode(
#         trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
#     )[0]

#     print("åŸå§‹å“åº”ï¼š", output_text)
#     return _extract_text_and_json(output_text)


# # ---------------- ç»Ÿä¸€å…¥å£ï¼šä¿æŒåŸå‡½æ•°ç­¾å ----------------
# def generate_robot_actions(
#     user_command: str,
#     image_input: Optional[np.ndarray] = None,
#     *,
#     # é€‰æ‹©åç«¯ï¼š"hf"ï¼ˆæœ¬åœ° Transformersï¼‰| "openai"ï¼ˆOpenAI-å…¼å®¹ï¼‰
#     backend: str = "openai",

#     # ---- HF(Qwen3-VL) é…ç½® ----
#     hf_model_name: str = "Qwen/Qwen3-VL-8B-Instruct",
#     hf_attn_implementation: Optional[str] = None,  # å¦‚ "flash_attention_2"
#     hf_max_new_tokens: int = 256,

#     # ---- OpenAI-å…¼å®¹é…ç½®ï¼ˆå¯é€‰ï¼‰----
#     api_key: str = "sk-21a42456345b4b6da1df9d0a08a4396c",
#     base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions",
#     model: str = "qwen-vl-plus",
# ) -> Dict[str, Any]:
#     """
#     ä½¿ç”¨ base64ï¼ˆOpenAI-å…¼å®¹ï¼‰æˆ–æœ¬åœ° Transformers(HF) çš„æ–¹å¼ä¸å¤šæ¨¡æ€æ¨¡å‹å¯¹è¯ï¼Œ
#     å¹¶è¿”å› {"response": è‡ªç„¶è¯­è¨€è¯´æ˜, "coordinates": JSON åæ ‡å¯¹è±¡}ã€‚
#     """
#     try:
#         if backend == "hf":
#             return _call_hf_qwen3(
#                 user_command=user_command,
#                 image_bgr=image_input,
#                 hf_model_name=hf_model_name,
#                 max_new_tokens=hf_max_new_tokens,
#                 attn_implementation=hf_attn_implementation,
#             )
#         elif backend == "openai":
#             img_b64 = encode_np_array(image_input) if image_input is not None else None
#             return _call_openai_compatible(
#                 user_command=user_command,
#                 image_b64=img_b64,
#                 model=model,
#                 api_key=api_key,
#                 base_url=base_url
#             )
#         else:
#             raise ValueError("æœªçŸ¥åç«¯ï¼šåº”ä¸º 'hf' æˆ– 'openai'ã€‚")

#     except Exception as e:
#         print(f"è¯·æ±‚å¤±è´¥ï¼š{e}")
#         return {"response": "å¤„ç†å¤±è´¥", "coordinates": {}}

# ----------------------- SAM åˆ†å‰²ç›¸å…³ -----------------------
def choose_model():
    """Initialize SAM predictor with proper parameters"""
    model_weight = 'sam_b.pt'
    overrides = dict(
        task='segment',
        mode='predict',
        # imgsz=1024,
        model=model_weight,
        conf=0.25,
        save=False
    )
    return SAMPredictor(overrides=overrides)

def process_sam_results(results):
    """Process SAM results to get mask and center point"""
    if not results or not results[0].masks:
        return None, None

    # Get first mask (assuming single object segmentation)
    mask = results[0].masks.data[0].cpu().numpy()
    mask = (mask > 0).astype(np.uint8) * 255

    # Find contour and center
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        return None, None

    M = cv2.moments(contours[0])
    if M["m00"] == 0:
        return None, mask

    cx = int(M["m10"] / M["m00"])
    cy = int(M["m01"] / M["m00"])
    return (cx, cy), mask


# ----------------------- è¯­éŸ³è¯†åˆ«ä¸ TTS -----------------------

# åˆå§‹åŒ–å…¨å±€æ¨¡å‹å˜é‡
_global_models = {}


def load_models():
    """åœ¨éœ€è¦æ—¶åŠ è½½æ¨¡å‹ï¼Œé¿å…å¯åŠ¨æ—¶å…¨éƒ¨åŠ è½½å ç”¨èµ„æº"""
    if not _global_models:
        print("ğŸ”„ æ­£åœ¨åŠ è½½ç¦»çº¿è¯­éŸ³æ¨¡å‹...")
        # åŠ è½½Whisperå°å‹æ¨¡å‹ (é€‚åˆä½ çš„6GBæ˜¾å­˜)
        # _global_models['asr'] = whisper.load_model("small")
        # _global_models['asr'] = whisper.load_model("tiny")
        # _global_models['asr'] = whisper.load_model("base")
        print("âœ… Whisperçš„baseæ¨¡å‹åŠ è½½å®Œæ¯•")

        try:
            import pyttsx3
            _global_models['tts_backup'] = pyttsx3.init()
            # é…ç½®TTS
            _global_models['tts_backup'].setProperty('rate', 160)  # è¯­é€Ÿ
            voices = _global_models['tts_backup'].getProperty('voices')
            for voice in voices:
                if 'chinese' in voice.name.lower() or 'zh' in voice.id.lower():
                    _global_models['tts_backup'].setProperty('voice', voice.id)
                    break
            print("âœ… TTS (pyttsx3) åˆå§‹åŒ–å®Œæ¯•")
        except Exception as e:
            print(f"âš ï¸  TTSåˆå§‹åŒ–å¤±è´¥: {e}")
            _global_models['tts_backup'] = None

    return _global_models


# éŸ³é¢‘å‚æ•°é…ç½®
samplerate = 16000
channels = 1
dtype = 'int16'
frame_duration = 0.2
frame_samples = int(frame_duration * samplerate)
silence_threshold = 250
silence_max_duration = 2.0
q = queue.Queue()


def rms(audio_frame):
    samples = np.frombuffer(audio_frame, dtype=np.int16)
    if samples.size == 0:
        return 0
    mean_square = np.mean(samples.astype(np.float32) ** 2)
    if np.isnan(mean_square) or mean_square < 1e-5:
        return 0
    return np.sqrt(mean_square)

def callback(indata, frames, time_info, status):
    if status:
        print("âš ï¸ çŠ¶æ€è­¦å‘Šï¼š", status)
    q.put(bytes(indata))

def recognize_speech():
    """å½•éŸ³å¹¶è¿”å›éŸ³é¢‘æ•°æ®ï¼ˆnumpy æ•°ç»„ï¼‰"""
    print("ğŸ™ï¸ å¯åŠ¨å½•éŸ³ï¼Œè¯·è¯´è¯...")
    # print("ğŸ’¡ è°ƒè¯•ä¿¡æ¯ï¼šæ­£åœ¨ç›‘æµ‹å®æ—¶éŸ³é‡ï¼ˆRMSï¼‰ï¼Œè¯·è§‚å¯Ÿä¸è¯´è¯æ—¶çš„åŸºç¡€å™ªéŸ³å€¼")
    audio_buffer = []
    is_speaking = False
    last_voice_time = time.time()

    with sd.RawInputStream(samplerate=samplerate, blocksize=frame_samples,
                           dtype=dtype, channels=channels, callback=callback):
        while True:
            frame = q.get()
            volume = rms(frame)
            current_time = time.time()

            # print(f"å®æ—¶éŸ³é‡ï¼ˆRMSï¼‰: {volume}") 

            if volume > silence_threshold:
                if not is_speaking:
                    print("ğŸ¤ æ£€æµ‹åˆ°è¯­éŸ³ï¼Œå¼€å§‹å½•éŸ³...")
                    is_speaking = True
                    audio_buffer = []
                audio_np = np.frombuffer(frame, dtype=np.int16)
                audio_buffer.append(audio_np)
                last_voice_time = current_time
            elif is_speaking and (current_time - last_voice_time > silence_max_duration):
                print("ğŸ›‘ åœæ­¢å½•éŸ³ï¼Œå‡†å¤‡è¯†åˆ«...")
                full_audio = np.concatenate(audio_buffer, axis=0)
                return full_audio
            elif not is_speaking and (current_time - last_voice_time > 10.0):
                print("ğŸ›‘ è¶…æ—¶ï¼šæœªæ£€æµ‹åˆ°è¯­éŸ³è¾“å…¥")
                return np.array([], dtype=np.int16)

def speech_to_text_offline(audio_data):
    """
    ä½¿ç”¨ç¦»çº¿Whisperæ¨¡å‹å°†å½•éŸ³æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬
    """
    print("ğŸ“¡ æ­£åœ¨è¿›è¡Œç¦»çº¿è¯­éŸ³è¯†åˆ«...")
    models = load_models()
    asr_model = models['asr']

    # ä¿å­˜ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
    temp_wav = "temp_audio.wav"
    write(temp_wav, samplerate, audio_data.astype(np.int16))

    try:
        # ä½¿ç”¨Whisperè¿›è¡Œè¯†åˆ«ï¼ŒæŒ‡å®šè¯­è¨€ä¸ºä¸­æ–‡ä»¥æé«˜ç²¾åº¦å’Œé€Ÿåº¦
        result = asr_model.transcribe(temp_wav, language="zh", fp16=torch.cuda.is_available())
        return result["text"].strip()
    except Exception as e:
        print(f"âŒ ç¦»çº¿è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
        return ""

def play_tts_offline(text):
    """
    ä½¿ç”¨ç¦»çº¿TTSæ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³å¹¶æ’­æ”¾
    """
    if not text:
        return
        
    print(f"ğŸ“¢ ç¦»çº¿TTSæ’­æ”¾: {text}")
    models = load_models()

    try:
        if models['tts_backup'] is not None:
            models['tts_backup'].say(text)
            models['tts_backup'].runAndWait()

    except Exception as e:
        print("âŒ æ— å¯ç”¨TTSå¼•æ“")


def voice_command_to_keyword():
    """
    è·å–è¯­éŸ³å‘½ä»¤å¹¶è½¬æ¢ä¸ºæ–‡æœ¬ã€‚
    ç›´æ¥è¿”å›è¯†åˆ«çš„æ–‡æœ¬æŒ‡ä»¤ã€‚
    """
    audio_data = recognize_speech()
    text = speech_to_text_offline(audio_data) # æ”¹ä¸ºè°ƒç”¨ç¦»çº¿ASR
    if not text:
        print("âš ï¸ æ²¡æœ‰è¯†åˆ«åˆ°æ–‡æœ¬")
        return ""
    print("ğŸ“ è¯†åˆ«æ–‡æœ¬ï¼š", text)
    # play_tts_offline(f"å·²æ”¶åˆ°æŒ‡ä»¤: {text}") # æ”¹ä¸ºè°ƒç”¨ç¦»çº¿TTS
    return text


# ----------------------- ä¸»æµç¨‹ï¼šå›¾åƒåˆ†å‰² -----------------------
def segment_image(image_input, output_mask='mask1.png'):
    # 1. ä½¿ç”¨æ–‡å­—è·å–ç›®æ ‡æŒ‡ä»¤
    print("ğŸ“ è¯·é€šè¿‡æ–‡å­—æè¿°ç›®æ ‡ç‰©ä½“åŠæŠ“å–æŒ‡ä»¤...")
    command_text = input("è¯·è¾“å…¥: ").strip()
    if not command_text:
        print("âš ï¸ æœªè¯†åˆ«åˆ°è¯­éŸ³æŒ‡ä»¤ï¼Œè¯·é‡è¯•ã€‚")
        return None
    print(f"âœ… è¯†åˆ«çš„è¯­éŸ³æŒ‡ä»¤ï¼š{command_text}")

    # # 1. ä½¿ç”¨è¯­éŸ³è·å–ç›®æ ‡æŒ‡ä»¤
    # print("ğŸ™ï¸ è¯·é€šè¿‡è¯­éŸ³æè¿°ç›®æ ‡ç‰©ä½“åŠæŠ“å–æŒ‡ä»¤...")
    # command_text = voice_command_to_keyword()
    # if not command_text:
    #     print("âš ï¸ æœªè¯†åˆ«åˆ°è¯­éŸ³æŒ‡ä»¤ï¼Œè¯·é‡è¯•ã€‚")
    #     return None
    # print(f"âœ… è¯†åˆ«çš„è¯­éŸ³æŒ‡ä»¤ï¼š{command_text}")

    # 2. é€šè¿‡å¤šæ¨¡æ€æ¨¡å‹è·å–æ£€æµ‹æ¡†
    result = generate_robot_actions(command_text, image_input)
    natural_response = result["response"]
    detection_info = result["coordinates"]
    print("è‡ªç„¶è¯­è¨€å›åº”ï¼š", natural_response)
    print("æ£€æµ‹åˆ°çš„ç‰©ä½“ä¿¡æ¯ï¼š", detection_info)

    # ä»…å¯¹æ¨¡å‹è¿”å›çš„è‡ªç„¶è¯­è¨€å›åº”æ’­æŠ¥
    play_tts_offline(natural_response)
    
    bbox = detection_info.get("bbox") if detection_info and "bbox" in detection_info else None
    
    # 3. å‡†å¤‡å›¾åƒä¾› SAM ä½¿ç”¨ï¼ˆè½¬æ¢ä¸º RGBï¼‰
    image_rgb = cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB)

    # 4. åˆå§‹åŒ– SAMï¼Œå¹¶è®¾ç½®å›¾åƒ
    predictor = choose_model()
    predictor.set_image(image_rgb)

    if bbox:
        results = predictor(bboxes=[bbox])
        center, mask = process_sam_results(results)
        print(f"âœ… è‡ªåŠ¨æ£€æµ‹åˆ°ç›®æ ‡,bbox:{bbox}")
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼Œè¯·ç‚¹å‡»å›¾åƒé€‰æ‹©å¯¹è±¡")
        cv2.namedWindow('Select Object', cv2.WINDOW_NORMAL)
        cv2.imshow('Select Object', image_input)
        point = []

        def click_handler(event, x, y, flags, param):
            if event == cv2.EVENT_LBUTTONDOWN:
                point.extend([x, y])
                print(f"ğŸ–±ï¸ ç‚¹å‡»åæ ‡ï¼š{x}, {y}")
                cv2.setMouseCallback('Select Object', lambda *args: None)

        cv2.setMouseCallback('Select Object', click_handler)
        while True:
            key = cv2.waitKey(100)
            if point:
                break
            if cv2.getWindowProperty('Select Object', cv2.WND_PROP_VISIBLE) < 1:
                print("âŒ çª—å£è¢«å…³é—­ï¼Œæœªè¿›è¡Œç‚¹å‡»")
                return None
        cv2.destroyAllWindows()
        results = predictor(points=[point], labels=[1])
        center, mask = process_sam_results(results)

    # 5. ä¿å­˜åˆ†å‰²æ©ç 
    if mask is not None:
        cv2.imwrite(output_mask, mask, [cv2.IMWRITE_PNG_BILEVEL, 1])
        print(f"âœ… åˆ†å‰²æ©ç å·²ä¿å­˜ï¼š{output_mask}")
    else:
        print("âš ï¸ åˆ†å‰²å¤±è´¥ï¼Œæœªç”Ÿæˆæ©ç ")

    return mask


# ----------------------- ä¸»ç¨‹åºå…¥å£ -----------------------
if __name__ == '__main__':
    seg_mask = segment_image('color_img_path.jpg')
    print("Segmentation result mask shape:", seg_mask.shape if seg_mask is not None else None)
